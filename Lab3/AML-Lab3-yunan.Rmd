---
title: "AML-Lab3-yunan"
author: "yunan Dong"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
library(ggplot2)
library(magrittr)
```



# A1 Q-learning

```{r a1-p1}

arrows <- c("^", ">", "v", "<")
action_deltas <- list(c(1,0), # up
                      c(0,1), # right
                      c(-1,0), # down
                      c(0,-1)) # left

vis_environment <- function(iterations=0, epsilon = 0.5, alpha = 0.1, gamma = 0.95, beta = 0){
  
  # Visualize an environment with rewards. 
  # Q-values for all actions are displayed on the edges of each tile.
  # The (greedy) policy for each state is also displayed.
  # 
  # Args:
  #   iterations, epsilon, alpha, gamma, beta (optional): for the figure title.
  #   reward_map (global variable): a HxW array containing the reward given at each state.
  #   q_table (global variable): a HxWx4 array containing Q-values for each state-action pair.
  #   H, W (global variables): environment dimensions.
  
  df <- expand.grid(x=1:H,y=1:W)
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,1],NA),df$x,df$y)
  df$val1 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,2],NA),df$x,df$y)
  df$val2 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,3],NA),df$x,df$y)
  df$val3 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,4],NA),df$x,df$y)
  df$val4 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) 
    ifelse(reward_map[x,y] == 0,arrows[GreedyPolicy(x,y)],reward_map[x,y]),df$x,df$y)
  df$val5 <- as.vector(foo)
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,max(q_table[x,y,]),
                                     ifelse(reward_map[x,y]<0,NA,reward_map[x,y])),df$x,df$y)
  df$val6 <- as.vector(foo)
  
  print(ggplot(df,aes(x = y,y = x)) +
          scale_fill_gradient(low = "white", high = "green", na.value = "red", name = "") +
          geom_tile(aes(fill=val6)) +
          geom_text(aes(label = val1),size = 4,nudge_y = .35,na.rm = TRUE) +
          geom_text(aes(label = val2),size = 4,nudge_x = .35,na.rm = TRUE) +
          geom_text(aes(label = val3),size = 4,nudge_y = -.35,na.rm = TRUE) +
          geom_text(aes(label = val4),size = 4,nudge_x = -.35,na.rm = TRUE) +
          geom_text(aes(label = val5),size = 10) +
          geom_tile(fill = 'transparent', colour = 'black') + 
          ggtitle(paste("Q-table after ",iterations," iterations\n",
                        "(epsilon = ",epsilon,", alpha = ",alpha,"gamma = ",gamma,", beta = ",beta,")")) +
          theme(plot.title = element_text(hjust = 0.5)) +
          scale_x_continuous(breaks = c(1:W),labels = c(1:W)) +
          scale_y_continuous(breaks = c(1:H),labels = c(1:H)))
  
}

#a1-p2-GreedyPolicy}------------------------------------------------------------------------------

GreedyPolicy <- function(x, y
                         ){
  
  # Get a greedy action for state (x,y) from q_table.
  #
  # Args:
  #   x, y: state coordinates.
  #   q_table (global variable): a HxWx4 array containing Q-values for each state-action pair.
  # 
  # Returns:
  #   An action, i.e. integer in {1,2,3,4}.
  #number_action =4  #------------------------------------------
  
  # Your code here.
  q_values = q_table[x,y,]
  max_q = max(q_values)
  max_q_index = which(max_q == q_values)
  
  # action only from greedy policy
  action = ifelse(length(max_q_index) > 1, sample(max_q_index, 1),max_q_index)
  
  
  return(action)
}



#a1-p3-EpsilonGreedyPolicy

EpsilonGreedyPolicy <- function(x, y, epsilon){
  
  # Get an epsilon-greedy action for state (x,y) from q_table.
  #
  # Args:
  #   x, y: state coordinates.
  #   epsilon: probability of acting randomly.
  # 
  # Returns:
  #   An action, i.e. integer in {1,2,3,4}.
  
  # Your code here.
  action = ifelse(runif(1)>epsilon, GreedyPolicy(x,y), sample(1:4, 1))
  
  return(action)
}

#a1-p4-transition_model---------------------------------------------------------------------------------


### find the materials to read
transition_model <- function(x, y, action, beta){
  
  # Computes the new state after given action is taken. The agent will follow the action 
  # with probability (1-beta) and slip to the right or left with probability beta/2 each.
  # 
  # Args:
  #   x, y: state coordinates.
  #   action: which action the agent takes (in {1,2,3,4}).
  #   beta: probability of the agent slipping to the side when trying to move.
  #   H, W (global variables): environment dimensions.
  # 
  # Returns:
  #   The new state after the action has been taken.
  
  delta <- sample(-1:1, size = 1, prob = c(0.5*beta,1-beta,0.5*beta))
  final_action <- ((action + delta + 3) %% 4) + 1   ### think about it ？
  foo <- c(x,y) + unlist(action_deltas[final_action])     ### action_deltas, see...？
  foo <- pmax(c(1,1),pmin(foo,c(H,W)))   ### https://statisticsglobe.com/pmax-pmin-r-function-example/
  
  return (foo)
}

# a1-p5-q_learning}-----------------------------------------------------------------------------------------------------------------

q_learning <- function(start_state, epsilon = 0.5, alpha = 0.1, gamma = 0.95, 
                       beta = 0){
  
  # Perform one episode of Q-learning. The agent should move around in the 
  # environment using the given transition model and update the Q-table.
  # The episode ends when the agent reaches a terminal state.
  # 
  # Args:
  #   start_state: array with two entries, describing the starting position of the agent.
  #   epsilon (optional): probability of acting greedily.
  #   alpha (optional): learning rate.
  #   gamma (optional): discount factor.
  #   beta (optional): slipping factor.
  #   reward_map (global variable): a HxW array containing the reward given at each state.
  #   q_table (global variable): a HxWx4 array containing Q-values for each state-action pair.
  # 
  # Returns:
  #   reward: reward received in the episode.
  #   correction: sum of the temporal difference correction terms over the episode.
  #   q_table (global variable): Recall that R passes arguments by value. So, q_table being
  #   a global variable can be modified with the superassigment operator <<-.
  
  # Your code here.
  
  episode_correction = 0   # 
  repeat{
    # Follow policy, execute action, get reward.
    action = EpsilonGreedyPolicy(start_state[1],start_state[2],epsilon) # Follow policy or GreedyPolicy(x,y) , never mind!!!
    new_state =  transition_model(start_state[1],start_state[2],action,beta) # execute action
    reward = reward_map[new_state[1],new_state[2]] # get reward.
    
    # Q-table update.
    Q_old = q_table[start_state[1],start_state[2],action]
    Q_max=max(q_table[new_state[1],new_state[2],])
    temporal_difference = reward + gamma * Q_max - Q_old
    q_table[start_state[1],start_state[2],action] <<- Q_old + alpha * temporal_difference  
    
    # accumulating correction for returning
    episode_correction = episode_correction + temporal_difference
   
    # updating state
    start_state = new_state
      
    
    if(reward!=0)
      # End episode.
      return (c(reward,episode_correction))
  }
  
}


```


# A2 Environment A

```{r}

# Environment A (learning)

H <- 5
W <- 7

reward_map <- matrix(0, nrow = H, ncol = W)
reward_map[3,6] <- 10
reward_map[2:4,3] <- -1

q_table <- array(0,dim = c(H,W,4))

vis_environment()

for(i in 1:10000){
  foo <- q_learning(start_state = c(3,1))
  
  if(any(i==c(10,100,1000,10000)))
    vis_environment(i)
}


```




**What has the agent learned after the first 10 episodes ?**

After running several times of the code, in the first 10 episodes, no significant path is learned 


**– Is the final greedy policy (after 10000 episodes) optimal for all states, i.e. not only for the initial state ? Why / Why not ?**

After running several times of the code, sometimes an optimal one for all states can be achieved, but due to the randomness in EpsilonGreedyPolicy and transition_model, it is probable that some grids are not gone through sufficiently


**Do the learned values in the Q-table reflect the fact that there are multiple paths (above and below the negative rewards) to get to the positive reward ? If not, what could be done to make it happen ?**

After running several times, I found that there are multiple paths, e.g. in starting point(3,1), the next path can be on the left or the right.





# Environment B 



```{r}

H <- 7
W <- 8

reward_map <- matrix(0, nrow = H, ncol = W)
reward_map[1,] <- -1
reward_map[7,] <- -1
reward_map[4,5] <- 5
reward_map[4,8] <- 10

q_table <- array(0,dim = c(H,W,4))

vis_environment()

MovingAverage <- function(x, n){
  
  cx <- c(0,cumsum(x))
  rsum <- (cx[(n+1):length(cx)] - cx[1:(length(cx) - n)]) / n
  
  return (rsum)
}

for(j in c(0.5,0.75,0.95)){
  q_table <- array(0,dim = c(H,W,4))
  reward <- NULL
  correction <- NULL
  
  for(i in 1:30000){
    foo <- q_learning(gamma = j, start_state = c(4,1))
    reward <- c(reward,foo[1])
    correction <- c(correction,foo[2])
  }
  
  vis_environment(i, gamma = j)
  plot(MovingAverage(reward,100),type = "l")
  plot(MovingAverage(correction,100),type = "l")
}

for(j in c(0.5,0.75,0.95)){
  q_table <- array(0,dim = c(H,W,4))
  reward <- NULL
  correction <- NULL
  
  for(i in 1:30000){
    foo <- q_learning(epsilon = 0.1, gamma = j, start_state = c(4,1))
    reward <- c(reward,foo[1])
    correction <- c(correction,foo[2])
  }
  
  vis_environment(i, epsilon = 0.1, gamma = j)
  plot(MovingAverage(reward,100),type = "l")
  plot(MovingAverage(correction,100),type = "l")
}


```

The question shows the importance of keeping a good balance between exploitation and exploration, the former put emphasis on short-time benefit and the latter on long-term benefit. The related parameters here are epsilon and gamma.

Gamma is the discount rate of future rewards, when gamma=0, the agent only focuses on immediate benefit, when gamma=1, it evaluates each of action based on the total sum of future rewards(so it is likely to spend lots of time). 

Epsilon is used to choose exploration and exploitation randomly with the probability of epsilon and 1-epsilon,respectively.



When Gamma =(0.5,0.75,0.95) and epsilon=0.5, with gamma's increase, the long-term reward is aware of and the corresponding optimal path  is built finally.
When gamma =0.5 and 0.75, the reward and correction go through a relatively increase and small fluctuation, compared with the adjustment when gamma=0.95,in which a significant increase in reward and change in correction  



When epsilon=0.1, which means the agent mainly doing on exploitation, it is no hard to see that the sub-optimal is attained as a so-called ultimate goal







# Environment C

```{r envc}
H <- 3
W <- 6

reward_map <- matrix(0, nrow = H, ncol = W)
reward_map[1,2:5] <- -1
reward_map[1,6] <- 10

q_table <- array(0,dim = c(H,W,4))

vis_environment()

for(j in c(0,0.2,0.4,0.66)){
  q_table <- array(0,dim = c(H,W,4))
  
  for(i in 1:10000)
    foo <- q_learning(gamma = 0.6, beta = j, start_state = c(1,1))
  
  vis_environment(i, gamma = 0.6, beta = j)
}

```


The aim to use beta seems to use the randomness to decrease the probability of dropping local optimun , dead loop and so on.
however,beta, as the random factor of decided action, when it is too big, it can make the agent not pointed to the optimal state that should have pointed to, as shown in the last picture, when beta=0.66 





# Environment D


**Has the agent learned a good policy? Why / Why not ?**

all the grids are accessed, it can be seen to learn a good policy

**Could you have used the Q-learning algorithm to solve this task ?**

No, the goal of Q-learning should be fixed. the sampled goal should be at first built by some model






# Environment E

*Has the agent learned a good policy? Why / Why not ?*

No, the agent get lost the grids and it can't find the goal

*If the results obtained for environments D and E differ, explain why*

The agent in D can go to everywhere, but the one in E is limited by part of the grids,which is limited.



# Appendix

```{r ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}


```